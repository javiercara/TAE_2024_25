---
title: "Árboles de regresión: 1 regresor"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
---

# Introducción

# Arbol con un regresor cuantitativo

## Construcción del árbol

```{r cars}
library(rpart)
d = read.csv('datos/kidiq.csv')
```

```{r}
# method = "anova" para modelos de regresion
t1 = rpart(kid_score ~ mom_iq, data = d, method = "anova")
```

```{r}
plot(t1, margin = 0.02)
text(t1, cex = 0.75)
```

```{r}
print(t1)
```

Lo que devuelve la tabla es:

- Numero del nodo
- split: criterio para hacer la partición del nodo
- n: numero de datos que componen el nodo.
- deviance: RSS = $\sum(y_i - \hat{y}_i)^2$
- yval: predicción del nodo = $\bar{y}$
- un asterisco * para indicar un nodo terminal u hoja.

Por ejemplo, para el nodo raiz:

- Predicción:

```{r}
(yp_root = mean(d$kid_score))
```

- Deviance:

```{r}
( deviance_root = sum( (d$kid_score - yp_root)^2 ) )
```

Para el nodo 3:

- Datos que pertenecen al nodo 3:

```{r}
d3 = d[d$mom_iq >= 84.33641,]
```

- Predicción:

```{r}
(yp_3 = mean(d3$kid_score))
```

- Deviance:

```{r}
( deviance_3 = sum( (d3$kid_score - yp_3)^2 ) )
```

## Parámetros del árbol

```{r}
t2 = rpart(kid_score ~ mom_iq, data = d, method = "anova",
    control = rpart.control(minsplit = 10, minbucket = 5, cp = 0.05))
plot(t2, margin = 0.02)
text(t2, cex=.75)
```

control:

- minsplit: número mínimo de observaciones del nodo para que se divida en dos (por defecto, minsplit = 20).
- minbucket: número mínimo de observaciones en un nodo terminal u hoja (por defecto, minbucket = minsplit/3).
- maxdepth: se fija el nivel máximo de cualquier nodo del árbol, siendo 0 el nivel del nodo raiz.
- cp: complexity parameter. En árboles de regresión, para que un nodo se divida, el R2 tiene que incrementarse en más de una cantidad data. En este caso: [RSS(padre) - RSS(hijo1) - RSS(hijo2)]/RSS(raiz) > cp (por defecto, cp = 0.01).
- xval: número de validaciones cruzadas. Se utiliza para el podado.

```{r}
print(t2)
```

- Como vemos, en este caso el criterio que detiene el crecimiento del árbol es cp. Por ejemplo, el nodo 3 se ha dividido ya que 

```{r}
(118793.70 - 58829.52 - 48941.98)/180386.20
```

- que es mayor que el límite cp = 0.01.

- Podemos construir un arbol más *profundo*:

```{r}
t3 = rpart(kid_score ~ mom_iq, data = d, method = "anova",
    control = rpart.control(minsplit = 10, minbucket = 5, cp = 0.0069))
plot(t3, margin = 0.02)
text(t3, cex=.75)
```

```{r}
print(t3)
```

- vemos que el nodo 7 en t2 no se dividía pero en t3 si se divide ya que:

```{r}
(48941.980 - 38970.810 - 8230.786)/180386.200
```

- De nuevo cp es el parámetro más restrictivo.

## Residuos

```{r}
plot(residuals(t3))
```

- El R2 se define a manera análoga a regresión

$$
R^2 = 1 - \frac{RSS}{TSS}
$$

- donde  RSS = $\sum e_i^2 = \sum deviance(hoja_i)$ y TSS = deviance(root)

- Se denomina error relativo al cociente RSS/TSS. Y la X indica que se ha calculado mediante validación cruzada.

```{r}
par(mfrow = c(1,2))
rsq.rpart(t3)
```

- Appatent: R2 calculado con la formula (1 - RSS/TSS)
- X Relative: R2 calculado con validación cruzada (como vemos, el R2 cuadrado con validación cruzada es menor que el apparent ya que uno esta calculado en los datos train y otro en los datos test).
- X relative error: 1 - X Relative, es decir, RSS/TSS. Está calculado con validación cruzada. Se dibuja el intervalo +/- SE calculado con validación cruzada.

# Podado

- Los árboles que hemos visto se construyen de arriba hacia abajo, desde el nodo raiz hasta las hojas. Otra estrategia es construir un arbol muy profundo y luego podarlo. Construiriamos el arbol, por tanto, de abajo hacia arriba.

- Primero construimos un arbol profundo:

```{r}
t4 = rpart(kid_score ~ mom_iq, data = d, method = "anova",
    control = rpart.control(minsplit = 2, cp = 0.005))
plot(t4, margin = 0.02)
text(t4, cex=.75)
```

- Utilizando validación cruzada (el numero de validaciones viene dado por el parámetro xval), se determina el arbol con un determinado numero de hojas que tenga el mayor R2, o de manera equivalente, el menor error relativo.  

```{r}
t4_printcp = printcp(t4) # lo guardamos en una variable para utilizarlo despues
```

- Salida de printcp():
    - col2: el valor de cp que hay que utilizar para obtener ese árbol
    - col3: numero de divisiones del arbol obtenido. Numero de hojas = numero divisiones + 1
    - col4: error relativo del arbol podado, RSS/TSS
    - col5: error relativo calculado con validación cruzada.
    - col6: desviación típica de xerror
- También se puede utilizar plotcp():

```{r}
plotcp(t4)
```

- A veces este gráfico tiene un mínimo, por lo que deberíamos seleccionar ese arbol. En caso contrario, elegimos el tamaño donde el error se estabilice.

- Según el gráfico y la tabla anterior, un arbol de 3 hojas (nsplit  =2) parece razonable. 

```{r}
(t4_cp = t4_printcp[3,"CP"])
```

- Ahora podamos el arbol:

```{r}
t4_prune = prune(t4, cp = t4_cp)
plot(t4_prune, margin = 0.02)
text(t4_prune, cex=.75)
```

En este caso se ha obtenido la misma solución que construyendo el árbol con los parámetros por defecto.

# Prediccion

```{r}
xp = data.frame(mom_iq = 95)
predict(t4_prune, newdata = xp)
```

- Mirando el arbol se puede verificar fácilmente la predicción.

